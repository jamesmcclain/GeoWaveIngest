```
% docker run -it --rm --net=geowave -v $SPARK_HOME:/spark:ro -v $(pwd)/raster-peek/target/scala-2.11:/jars:ro -v /tmp/tif:/tmp/tif java:openjdk-8u72-jdk  [203/716]
root@11ff8d62caa7:/# /spark/bin/spark-submit --master='local[1]' --conf 'spark.driver.memory=1G' --class com.example.raster.RasterDisgorge /jars/raster-peek-assembly-0.jar leader instance root password gwRaster
01 Jun 23:55:43 WARN [util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
01 Jun 23:55:47 WARN [client.ClientConfiguration] - Found no client.conf in default paths. Using default client configuration values.
01 Jun 23:55:49 WARN [index.PersistenceUtils] - error creating class: could not create class mil.nga.giat.geowave.adapter.raster.query.IndexOnlySpatialQuery
java.lang.NoSuchMethodException: mil.nga.giat.geowave.adapter.raster.query.IndexOnlySpatialQuery.<init>()
        at java.lang.Class.getConstructor0(Class.java:3082)
        at java.lang.Class.getDeclaredConstructor(Class.java:2178)
        at mil.nga.giat.geowave.core.index.PersistenceUtils.classFactory(PersistenceUtils.java:121)
        at mil.nga.giat.geowave.core.index.PersistenceUtils.fromBinary(PersistenceUtils.java:91)
        at mil.nga.giat.geowave.mapreduce.input.GeoWaveInputConfigurator.getQueryInternal(GeoWaveInputConfigurator.java:41)
        at mil.nga.giat.geowave.mapreduce.input.GeoWaveInputConfigurator.getQuery(GeoWaveInputConfigurator.java:117)
        at mil.nga.giat.geowave.mapreduce.input.GeoWaveInputFormat.getQuery(GeoWaveInputFormat.java:121)
        at mil.nga.giat.geowave.mapreduce.input.GeoWaveInputFormat.getSplits(GeoWaveInputFormat.java:287)
        at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:120)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:912)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:910)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.foreach(RDD.scala:910)
        at com.example.raster.RasterDisgorge$.peek(RasterDisgorge.scala:86)
        at com.example.raster.RasterDisgorge$.main(RasterDisgorge.scala:116)
        at com.example.raster.RasterDisgorge.main(RasterDisgorge.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
01 Jun 23:55:51 WARN [index.PersistenceUtils] - error creating class: could not create class mil.nga.giat.geowave.adapter.raster.query.IndexOnlySpatialQuery               [164/716]
java.lang.NoSuchMethodException: mil.nga.giat.geowave.adapter.raster.query.IndexOnlySpatialQuery.<init>()
        at java.lang.Class.getConstructor0(Class.java:3082)
        at java.lang.Class.getDeclaredConstructor(Class.java:2178)
        at mil.nga.giat.geowave.core.index.PersistenceUtils.classFactory(PersistenceUtils.java:121)
        at mil.nga.giat.geowave.core.index.PersistenceUtils.fromBinary(PersistenceUtils.java:91)
        at mil.nga.giat.geowave.mapreduce.input.GeoWaveInputConfigurator.getQueryInternal(GeoWaveInputConfigurator.java:41)
        at mil.nga.giat.geowave.mapreduce.input.GeoWaveInputConfigurator.getQuery(GeoWaveInputConfigurator.java:117)
        at mil.nga.giat.geowave.mapreduce.input.GeoWaveInputFormat.getQuery(GeoWaveInputFormat.java:121)
        at mil.nga.giat.geowave.mapreduce.input.GeoWaveInputFormat.createRecordReader(GeoWaveInputFormat.java:211)
        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:156)
        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:129)
        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:64)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'GeoServerResourceLoader', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'GeoServerResourceLoader', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'ExtensionFilter', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'ExtensionProvider', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'ExtensionFilter', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'GeoServerResourceLoader', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'GeoServerResourceLoader', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'ExtensionFilter', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'ExtensionProvider', but ApplicationContext is unset.
01 Jun 23:55:52 WARN [geoserver.platform] - Extension lookup 'ExtensionFilter', but ApplicationContext is unset.
01 Jun 23:55:52 ERROR [executor.Executor] - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.IllegalArgumentException: value doesn't fit                                                                                                                      [132/716]
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)
        at com.google.uzaygezen.core.LongBitVector.copyFrom(LongBitVector.java:347)
        at com.google.uzaygezen.core.LongBitVector.copyFromBigEndian(LongBitVector.java:581)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.PrimitiveHilbertSFCOperations.indexInverse(PrimitiveHilbertSFCOperations.java:193)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.PrimitiveHilbertSFCOperations.convertFromHilbert(PrimitiveHilbertSFCOperations.java:171)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.HilbertSFC.getRanges(HilbertSFC.java:273)
        at mil.nga.giat.geowave.core.index.sfc.tiered.TieredSFCIndexStrategy.getRangeForId(TieredSFCIndexStrategy.java:259)
        at mil.nga.giat.geowave.core.index.sfc.tiered.SingleTierSubStrategy.getRangeForId(SingleTierSubStrategy.java:73)
        at mil.nga.giat.geowave.core.index.sfc.tiered.SingleTierSubStrategy.getRangeForId(SingleTierSubStrategy.java:27)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.getCoverageFromRasterTile(RasterDataAdapter.java:801)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.decode(RasterDataAdapter.java:791)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.decode(RasterDataAdapter.java:130)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRow(AccumuloUtils.java:336)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRowObj(AccumuloUtils.java:192)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRow(AccumuloUtils.java:172)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.decodeRow(InputFormatIteratorWrapper.java:76)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.findNext(InputFormatIteratorWrapper.java:58)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.hasNext(InputFormatIteratorWrapper.java:112)
        at mil.nga.giat.geowave.core.store.CloseableIteratorWrapper.hasNext(CloseableIteratorWrapper.java:46)
        at mil.nga.giat.geowave.datastore.accumulo.mapreduce.GeoWaveAccumuloRecordReader$3.hasNext(GeoWaveAccumuloRecordReader.java:372)
        at mil.nga.giat.geowave.core.store.CloseableIteratorWrapper.hasNext(CloseableIteratorWrapper.java:46)
        at mil.nga.giat.geowave.datastore.accumulo.mapreduce.GeoWaveAccumuloRecordReader.nextKeyValue(GeoWaveAccumuloRecordReader.java:217)
        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:168)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$32.apply(RDD.scala:912)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$32.apply(RDD.scala:912)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
01 Jun 23:55:52 WARN [scheduler.TaskSetManager] - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.IllegalArgumentException: value doesn't fit                      [95/716]
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)
        at com.google.uzaygezen.core.LongBitVector.copyFrom(LongBitVector.java:347)
        at com.google.uzaygezen.core.LongBitVector.copyFromBigEndian(LongBitVector.java:581)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.PrimitiveHilbertSFCOperations.indexInverse(PrimitiveHilbertSFCOperations.java:193)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.PrimitiveHilbertSFCOperations.convertFromHilbert(PrimitiveHilbertSFCOperations.java:171)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.HilbertSFC.getRanges(HilbertSFC.java:273)
        at mil.nga.giat.geowave.core.index.sfc.tiered.TieredSFCIndexStrategy.getRangeForId(TieredSFCIndexStrategy.java:259)
        at mil.nga.giat.geowave.core.index.sfc.tiered.SingleTierSubStrategy.getRangeForId(SingleTierSubStrategy.java:73)
        at mil.nga.giat.geowave.core.index.sfc.tiered.SingleTierSubStrategy.getRangeForId(SingleTierSubStrategy.java:27)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.getCoverageFromRasterTile(RasterDataAdapter.java:801)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.decode(RasterDataAdapter.java:791)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.decode(RasterDataAdapter.java:130)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRow(AccumuloUtils.java:336)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRowObj(AccumuloUtils.java:192)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRow(AccumuloUtils.java:172)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.decodeRow(InputFormatIteratorWrapper.java:76)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.findNext(InputFormatIteratorWrapper.java:58)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.hasNext(InputFormatIteratorWrapper.java:112)
        at mil.nga.giat.geowave.core.store.CloseableIteratorWrapper.hasNext(CloseableIteratorWrapper.java:46)
        at mil.nga.giat.geowave.datastore.accumulo.mapreduce.GeoWaveAccumuloRecordReader$3.hasNext(GeoWaveAccumuloRecordReader.java:372)
        at mil.nga.giat.geowave.core.store.CloseableIteratorWrapper.hasNext(CloseableIteratorWrapper.java:46)
        at mil.nga.giat.geowave.datastore.accumulo.mapreduce.GeoWaveAccumuloRecordReader.nextKeyValue(GeoWaveAccumuloRecordReader.java:217)
        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:168)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$32.apply(RDD.scala:912)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$32.apply(RDD.scala:912)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

01 Jun 23:55:52 ERROR [scheduler.TaskSetManager] - Task 0 in stage 0.0 failed 1 times; aborting job                                                                         [57/716]
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (T
ID 0, localhost): java.lang.IllegalArgumentException: value doesn't fit
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)
        at com.google.uzaygezen.core.LongBitVector.copyFrom(LongBitVector.java:347)
        at com.google.uzaygezen.core.LongBitVector.copyFromBigEndian(LongBitVector.java:581)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.PrimitiveHilbertSFCOperations.indexInverse(PrimitiveHilbertSFCOperations.java:193)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.PrimitiveHilbertSFCOperations.convertFromHilbert(PrimitiveHilbertSFCOperations.java:171)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.HilbertSFC.getRanges(HilbertSFC.java:273)
        at mil.nga.giat.geowave.core.index.sfc.tiered.TieredSFCIndexStrategy.getRangeForId(TieredSFCIndexStrategy.java:259)
        at mil.nga.giat.geowave.core.index.sfc.tiered.SingleTierSubStrategy.getRangeForId(SingleTierSubStrategy.java:73)
        at mil.nga.giat.geowave.core.index.sfc.tiered.SingleTierSubStrategy.getRangeForId(SingleTierSubStrategy.java:27)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.getCoverageFromRasterTile(RasterDataAdapter.java:801)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.decode(RasterDataAdapter.java:791)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.decode(RasterDataAdapter.java:130)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRow(AccumuloUtils.java:336)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRowObj(AccumuloUtils.java:192)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRow(AccumuloUtils.java:172)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.decodeRow(InputFormatIteratorWrapper.java:76)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.findNext(InputFormatIteratorWrapper.java:58)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.hasNext(InputFormatIteratorWrapper.java:112)
        at mil.nga.giat.geowave.core.store.CloseableIteratorWrapper.hasNext(CloseableIteratorWrapper.java:46)
        at mil.nga.giat.geowave.datastore.accumulo.mapreduce.GeoWaveAccumuloRecordReader$3.hasNext(GeoWaveAccumuloRecordReader.java:372)
        at mil.nga.giat.geowave.core.store.CloseableIteratorWrapper.hasNext(CloseableIteratorWrapper.java:46)
        at mil.nga.giat.geowave.datastore.accumulo.mapreduce.GeoWaveAccumuloRecordReader.nextKeyValue(GeoWaveAccumuloRecordReader.java:217)
        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:168)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$32.apply(RDD.scala:912)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$32.apply(RDD.scala:912)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:                                                                                                                                                          [17/716]
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:912)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:910)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.foreach(RDD.scala:910)
        at com.example.raster.RasterDisgorge$.peek(RasterDisgorge.scala:86)
        at com.example.raster.RasterDisgorge$.main(RasterDisgorge.scala:116)
        at com.example.raster.RasterDisgorge.main(RasterDisgorge.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.IllegalArgumentException: value doesn't fit
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)
        at com.google.uzaygezen.core.LongBitVector.copyFrom(LongBitVector.java:347)
        at com.google.uzaygezen.core.LongBitVector.copyFromBigEndian(LongBitVector.java:581)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.PrimitiveHilbertSFCOperations.indexInverse(PrimitiveHilbertSFCOperations.java:193)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.PrimitiveHilbertSFCOperations.convertFromHilbert(PrimitiveHilbertSFCOperations.java:171)
        at mil.nga.giat.geowave.core.index.sfc.hilbert.HilbertSFC.getRanges(HilbertSFC.java:273)
        at mil.nga.giat.geowave.core.index.sfc.tiered.TieredSFCIndexStrategy.getRangeForId(TieredSFCIndexStrategy.java:259)
        at mil.nga.giat.geowave.core.index.sfc.tiered.SingleTierSubStrategy.getRangeForId(SingleTierSubStrategy.java:73)
        at mil.nga.giat.geowave.core.index.sfc.tiered.SingleTierSubStrategy.getRangeForId(SingleTierSubStrategy.java:27)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.getCoverageFromRasterTile(RasterDataAdapter.java:801)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.decode(RasterDataAdapter.java:791)
        at mil.nga.giat.geowave.adapter.raster.adapter.RasterDataAdapter.decode(RasterDataAdapter.java:130)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRow(AccumuloUtils.java:336)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRowObj(AccumuloUtils.java:192)
        at mil.nga.giat.geowave.datastore.accumulo.util.AccumuloUtils.decodeRow(AccumuloUtils.java:172)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.decodeRow(InputFormatIteratorWrapper.java:76)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.findNext(InputFormatIteratorWrapper.java:58)
        at mil.nga.giat.geowave.datastore.accumulo.util.InputFormatIteratorWrapper.hasNext(InputFormatIteratorWrapper.java:112)
        at mil.nga.giat.geowave.core.store.CloseableIteratorWrapper.hasNext(CloseableIteratorWrapper.java:46)
        at mil.nga.giat.geowave.datastore.accumulo.mapreduce.GeoWaveAccumuloRecordReader$3.hasNext(GeoWaveAccumuloRecordReader.java:372)
        at mil.nga.giat.geowave.core.store.CloseableIteratorWrapper.hasNext(CloseableIteratorWrapper.java:46)
        at mil.nga.giat.geowave.datastore.accumulo.mapreduce.GeoWaveAccumuloRecordReader.nextKeyValue(GeoWaveAccumuloRecordReader.java:217)
        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:168)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$32.apply(RDD.scala:912)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$32.apply(RDD.scala:912)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
